import {lambdaSend} from "../../aws/handlers"
import {Function} from "sst/node/function";
import * as S from '@gnothi/schemas'
import {TextsParamsMatch} from "../errors";
import {Config} from 'sst/node/config'
import {v4 as uuid} from 'uuid'
import {sendInsight} from "./utils";
import {completion, Prompt} from "./openai";
import {getSummary} from '@gnothi/schemas/entries'
import {insights_themes_response} from '@gnothi/schemas/insights'
const r = S.Routes.routes
import {FnContext} from '../../routes/types'

type ThemesOut = insights_themes_response['themes']

export interface SummarizeEntryIn {
  text: string
  paras: string[]
  usePrompt: boolean
}
export interface SummarizeEntryOut {
  title: string
  paras: string[]
  body: {
    text: string
    emotion: string
    keywords: string[]
  }
}

interface FnIn {
  texts: string[]
}
export type FnOut = {
  title: string
  summary: string
  keywords: string[]
  emotion: string
}
type ParsedCompletion = {
  title: string
  summary: string
  emotion: string
  themes: {
    word: string
    keywords: string[]
    summary: string
  }[]
}

const TS_FORMAT = `\`\`\`typescript
{
  "title": string // a short headline 
  "summary": string // a summary of the entry, highlighting the key points
  "emotion": "anger"|"disgust"|"fear"|"joy"|"neutral"|"sadness"|"surprise"
  "themes": Array<{ // an array of 1-3 core themes present in the entry
    "title": string // 1-3 words for this theme
    "summary": string // a sentence summarizing the theme
    "keywords": string[] // an array of words or key-phrases in the theme
  }>
}
\`\`\``
const PROMPT = `Between >>> and <<< is a journal entry. Convert it to JSON with the following format:
${TS_FORMAT}
>>> [JOURNAL] <<<`

// this was generated by GPT, clean it up when I get a chance
function parseCompletion(originalEntry: string, llmOutput: string): ParsedCompletion {
  try {
    return JSON.parse(llmOutput)
  } catch (e) {
    console.error("LLM failed to output valid JSON", {originalEntry, llmOutput})
    return {
      // match up to the first punctuation or new-line
      title: originalEntry.match(/^(.*?)[\n\r,.!?]/)?.[1] || originalEntry,
      summary: originalEntry,
      emotion: "neutral",
      themes: []
    }
  }
}

function squashTexts(lines: string[]): string {
  // join all lines together, and regex replace all \n with space
  return lines.join('\n').replace(/[\n\r]+/g, ' ')
}

/**
 * I've had a hell of a time with huggingface summarizers. They lean too heavily on
 * the training data, and everything sounds like news. I've played with hyperparameters
 * till I'm blue in the face, and I'm giving up for now and using OpenAI TLDR instead.
 */
export async function summarizeOpenai({texts}: FnIn): Promise<ParsedCompletion> {
  // replace all \n with ' ' before master-prompting
  const squashed = squashTexts(texts)

  // Even with any of keywords,emotion,summary disabled; crafting our combinedPrompt over time makes this easier to
  // get right, and we can just remove the results before returning

  const messages: Prompt = [
    {role: "system", content: "You are a helpful assistant."},
    {role: "user", content: PROMPT.replace("[JOURNAL]", squashed)},
  ]
  const response = await completion({
    model: "gpt-3.5-turbo-16k",
    max_tokens: 512,
    prompt: messages
  })

  return parseCompletion(squashed, response)
}

export async function summarizeLambda({texts}: FnIn): Promise<ParsedCompletion> {
  // replace all \n with ' ' before master-prompting
  const squashed = squashTexts(texts)
  const fnName = Config.FN_SUMMARIZE_NAME
  const res = await lambdaSend<LambdaOut>({texts: squashed}, fnName, "RequestResponse")
  let txt: string = res.Payload.data[0].generated_text
  txt = txt.replaceAll('---', '\n')
  debugger
  return parseCompletion(squashed, txt)
}

/**
 * Helper function for summarize on insights page
 */
interface SummarizeInsights {
  entries: S.Entries.Entry[]
  context: FnContext,
  usePrompt: boolean
}
export async function summarizeInsights({context, entries, usePrompt}: SummarizeInsights) {
  async function sendInsights(parsed: ParsedCompletion): Promise<FnOut> {
    const themes: ThemesOut = parsed.themes
    const keywords = themes.map(t => t.keywords).flat()
    await Promise.all([
      sendInsight(
        r.insights_summarize_response,
        {...parsed, keywords},
        context
      ),
      sendInsight(
        r.insights_themes_response,
        parsed,
        context
      ),
    ])
  }

  if (entries.length === 0) {
    return sendInsights({
      title: "",
      summary: "Nothing to summarize",
      emotion: "neutral",
      themes: []
    })
  }
  const fn = usePrompt ? summarizeOpenai : summarizeLambda
  const parsed = await fn({
    // summarize summaries, NOT full originals (to reduce token max)
    texts: entries.map(getSummary),
  })
  return sendInsights(parsed)
}

interface SuggestNextEntry {
  context: FnContext
  usePrompt: boolean
  entries: S.Entries.Entry[]
  view: string
}
export async function suggestNextEntry({entries, context, usePrompt, view}: SuggestNextEntry) {
  if (!usePrompt) { return }
  if (!entries?.length) {return}
  const text = squashTexts(entries.map(getSummary))
  const response = await completion({
    model: "gpt-3.5-turbo-16k",
    max_tokens: 256,
    prompt: `Below in triple quotes are my previous journal entries. What should I journal about next to explore more deeply the deeper themes in these entries?\n"""${text}"""`
  })
  return sendInsight(
    r.insights_nextentry_response,
    {text: response},
    context
  )
}


export async function summarizeEntry({text, paras, usePrompt}: SummarizeEntryIn): Promise<SummarizeEntryOut> {
  // This shouldn't happen, but I haven't tested to ensure
  if (paras.length === 0) {
    throw "paras.length === 0, investigate"
  }

  const fn = usePrompt ? summarizeOpenai : summarizeLambda
  const parsed = await fn({
    // summarize summaries, NOT full originals (to reduce token max)
    texts: paras,
  })

  return {
    title: parsed.title,
    paras: [parsed.summary],
    body: {
      text: parsed.summary,
      emotion: parsed.emotion,
      keywords: parsed.themes.map(t => t.keywords).flat(),
    }
  }
}
