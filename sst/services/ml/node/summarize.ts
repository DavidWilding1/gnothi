import {lambdaSend} from "../../aws/handlers"
import {Function} from "sst/node/function";
import * as S from '@gnothi/schemas'
import {TextsParamsMatch} from "../errors";
import {Config} from 'sst/node/config'
import {v4 as uuid} from 'uuid'
import {sendInsight} from "./utils";
import {completion, Prompt} from "./openai";
import {getSummary} from '@gnothi/schemas/entries'
import {insights_themes_response} from '@gnothi/schemas/insights'
const r = S.Routes.routes
import {FnContext} from '../../routes/types'

type ThemesOut = insights_themes_response['themes']

export interface SummarizeEntryIn {
  text: string
  paras: string[]
  usePrompt: boolean
}
export interface SummarizeEntryOut {
  title: string
  paras: string[]
  body: {
    text: string
    emotion: string
    keywords: string[]
  }
}

interface FnIn {
  texts: string[]
}
export type FnOut = {
  title: string
  summary: string
  keywords: string[]
  emotion: string
}
type ParsedCompletion = {
  title: string
  summary: string
  emotion: string
  themes: {
    word: string
    keywords: string[]
    summary: string
  }[]
}
/*

 */

const combinedPrompt = `Provide \`Title\`, \`Summary\`, \`Emotion\`, and \`Themes\` for the following entry. Maintain the first person, if used in the journal. \`Themes\` follow this format: \`theme1[keyword1,keyword2]a summary of theme1|theme2[keyword1,keyword2]a summary of theme2\`. \`Emotion\` should be one of \`anger|disgust|fear|joy|neutral|sadness|surprise\`. Below is a sample output in this format, pay attention to the spacing and capitalization:
\`\`\`
Title:The singularity is near
Summary:Artificial intelligence is improving at a rapid clip. Large language models like GPT4 and Llama are increasingly capable, passing various human benchmarks like the Bar Exam, and even the Turing test. Stable Diffusion is able to create art which is indistinguishable from human creativity. Neural Link is coming soon, a harbinger of full-dive VR. Video games now have AI NPCs, which can talk and act creatively in their environments.
Emotion:surprise
Themes:Large Language Models[llm,gpt,openai]LLMs like GPT pass the Turing Test|AI Art[art,stable-diffusion,midjourney]AI art is near-perfect|Virtual Reality[vr,games,bci]VR is incorporating AI via AI NPCs, generative art, and Brain Computer Interfaces
\`\`\`

Now here's the article to generate output for:
\`\`\`
<journal>
\`\`\``

// this was generated by GPT, clean it up when I get a chance
function parseCompletion(output: string): ParsedCompletion {
  const [titleLine, summaryLine, emotionLine, themesLine] = output.split('\n');

  const title = titleLine.replace(/Title:\s*/, '');
  const summary = summaryLine.replace(/Summary:\s*/, '');
  let emotion = emotionLine.replace(/Emotion:\s*/, '') || ""
  emotion = emotion.toLowerCase().trim()

  const themeRegExp = /(\w+)\[([^\]]+)\](.*?)(?=\||$)/g;
  let match;
  const themes = [];

  while ((match = themeRegExp.exec(themesLine)) !== null) {
    const [, themeTitle, keywordsStr, themeSummary] = match;
    const keywords = keywordsStr.split(',');
    themes.push({
      word: themeTitle,
      keywords,
      summary: themeSummary.trim(),
    });
  }

  return {title, summary, emotion, themes};
}

function squashTexts(lines: string[]): string {
  // join all lines together, and regex replace all \n with space
  return lines.join('\n').replace(/[\n\r]+/g, ' ')
}

/**
 * I've had a hell of a time with huggingface summarizers. They lean too heavily on
 * the training data, and everything sounds like news. I've played with hyperparameters
 * till I'm blue in the face, and I'm giving up for now and using OpenAI TLDR instead.
 */
export async function summarizeOpenai({texts}: FnIn): Promise<ParsedCompletion> {
  // replace all \n with ' ' before master-prompting
  const squashed = squashTexts(texts)

  // Even with any of keywords,emotion,summary disabled; crafting our combinedPrompt over time makes this easier to
  // get right, and we can just remove the results before returning

  const messages: Prompt = [
    {role: "system", content: "You are a helpful assistant."},
    {role: "user", content: combinedPrompt.replace("<journal>", squashed)},
  ]
  const response = await completion({
    model: "gpt-3.5-turbo",
    max_tokens: 512,
    prompt: messages
  })

  return parseCompletion(response)
}

export async function summarizeLambda({texts}: FnIn): Promise<ParsedCompletion> {
  // replace all \n with ' ' before master-prompting
  const squashed = squashTexts(texts)
  const fnName = Config.FN_SUMMARIZE_NAME
  const res = await lambdaSend<LambdaOut>({texts: squashed}, fnName, "RequestResponse")
  let txt: string = res.Payload.data[0].generated_text
  txt = txt.replaceAll('---', '\n')

  // somtimes it generates Title, but not Summary
  if (txt.split('\n').length < 4 && !txt.includes('Summary:')) {
    // Replace the first punctuation character with a newline
    txt = txt.replace(/[.,;?!]/, '\nSummary:');
  }
  console.log({txt})

  return parseCompletion(txt)
}

/**
 * Helper function for summarize on insights page
 */
interface SummarizeInsights {
  entries: S.Entries.Entry[]
  context: FnContext,
  usePrompt: boolean
}
export async function summarizeInsights({context, entries, usePrompt}: SummarizeInsights) {
  async function sendInsights(parsed: ParsedCompletion): Promise<FnOut> {
    const themes: ThemesOut = parsed.themes
    const keywords = themes.map(t => t.keywords).flat()
    await Promise.all([
      sendInsight(
        r.insights_summarize_response,
        {...parsed, keywords},
        context
      ),
      sendInsight(
        r.insights_themes_response,
        parsed,
        context
      )
    ])
  }

  if (entries.length === 0) {
    return sendInsights({
      title: "",
      summary: "Nothing to summarize",
      emotion: "neutral",
      themes: []
    })
  }
  const fn = usePrompt ? summarizeOpenai : summarizeLambda
  const parsed = await fn({
    // summarize summaries, NOT full originals (to reduce token max)
    texts: entries.map(getSummary),
  })
  return sendInsights(parsed)
}


export async function summarizeEntry({text, paras, usePrompt}: SummarizeEntryIn): Promise<SummarizeEntryOut> {
  // This shouldn't happen, but I haven't tested to ensure
  if (paras.length === 0) {
    throw "paras.length === 0, investigate"
  }

  const fn = usePrompt ? summarizeOpenai : summarizeLambda
  const parsed = await fn({
    // summarize summaries, NOT full originals (to reduce token max)
    texts: paras,
  })

  return {
    title: parsed.title,
    paras: [parsed.summary],
    body: {
      text: parsed.summary,
      emotion: parsed.emotion,
      keywords: parsed.themes.map(t => t.keywords).flat(),
    }
  }
}
