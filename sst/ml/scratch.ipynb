{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3f9775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "\n",
    "objects = []\n",
    "articles = [\n",
    "    ['cbt', \"https://en.wikipedia.org/wiki/Cognitive_behavioral_therapy\"],\n",
    "    ['vr', \"https://en.wikipedia.org/wiki/Virtual_reality\"],\n",
    "    ['ai', \"https://en.wikipedia.org/wiki/Artificial_intelligence\"]\n",
    "]\n",
    "for [topic, url] in articles:\n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    paras = []\n",
    "    for para in soup.find(id=\"mw-content-text\").find_all('p'):\n",
    "        para = para.get_text()\n",
    "        para = para.replace('\\n', ' ')\n",
    "        para = re.sub(r\"\\[[0-9]+\\]\", \"\", para)\n",
    "        if len(para) < 10:\n",
    "            continue\n",
    "        # para = re.sub(r\"\\[[0-9]*\\]\", \"\", para)\n",
    "        if len(paras) == 0:\n",
    "            paras.append(para)\n",
    "        elif len(para) < 100:\n",
    "            paras[len(paras) - 1] += para\n",
    "        else:\n",
    "            paras.append(para)\n",
    "    for i, para in enumerate(paras):\n",
    "        doc = dict(\n",
    "            text=para,\n",
    "            title=f\"{topic}{i}\"\n",
    "        )\n",
    "        objects.append(doc)\n",
    "\n",
    "with open(\"mock_entries.json\", \"w\") as f:\n",
    "    f.write(json.dumps(objects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a4cc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = False\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "import json\n",
    "\n",
    "from haystack.document_stores.faiss import FAISSDocumentStore\n",
    "\n",
    "document_store = FAISSDocumentStore.load('./index.faiss', './index.config')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a015084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import DensePassageRetriever\n",
    "dpr_retriever = DensePassageRetriever(\n",
    "    document_store=document_store,\n",
    "    use_gpu=False\n",
    "    # query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n",
    "    # passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ef46c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from haystack.nodes import RAGenerator\n",
    "rag_generator = RAGenerator(\n",
    "    model_name_or_path=\"facebook/rag-sequence-nq\",\n",
    "    retriever=dpr_retriever,\n",
    "    top_k=3,\n",
    "    min_length=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53742695",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.pipelines import GenerativeQAPipeline\n",
    "\n",
    "from haystack.nodes import FARMReader\n",
    "from haystack.pipelines import ExtractiveQAPipeline\n",
    "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=False)\n",
    "pipe = ExtractiveQAPipeline(reader, dpr_retriever)\n",
    "result = pipe.run(query='What is CBT?', params={\"Retriever\": {\"top_k\": 5}})\n",
    "a = 1\n",
    "\n",
    "# # Change `minimum` to `medium` or `all` to raise the level of detail\n",
    "# print_answers(result, details=\"all\")\n",
    "print(result)\n",
    "return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "633b44ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from txtai.embeddings import Embeddings\n",
    "\n",
    "# Create embeddings model, backed by sentence-transformers & transformers\n",
    "embeddings = Embeddings({\"path\": \"sentence-transformers/nli-mpnet-base-v2\", \"content\":True})\n",
    "\n",
    "data = {\n",
    "  \"uid1\": {\"text\": \"US tops 5 million confirmed virus cases\", \"obj_id\": \"obj_id1\"},\n",
    "  \"uid1\": {\"text\": \"Canada's last fully intact ice shelf has suddenly collapsed\", \"obj_id\": \"obj_id2\"},\n",
    "  \"uid1\": {\"text\": \"forming a Manhattan-sized iceberg\", \"obj_id\": \"obj_id3\"},\n",
    "  \"uid1\": {\"text\": \"Beijing mobilises invasion craft along coast as Taiwan tensions escalate\", \"obj_id\": \"obj_id4\"},\n",
    "  \"uid1\": {\"text\": \"The National Park Service warns against sacrificing slower friends \", \"obj_id\": \"obj_id5\"},\n",
    "  \"uid1\": {\"text\": \"in a bear attack\", \"obj_id\": \"obj_id6\"},\n",
    "  \"uid1\": {\"text\": \"Maine man wins $1M from $25 lottery ticket\", \"obj_id\": \"obj_id7\"},\n",
    "  \"uid1\": {\"text\": \"Make huge profits without work, earn up to $100,000 a day\", \"obj_id\": \"obj_id8\"}\n",
    "}\n",
    "\n",
    "embeddings.index([\n",
    "    (uid, obj, None) \n",
    "    for uid, obj in data.items()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ccda43b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IntegrityError",
     "evalue": "UNIQUE constraint failed: sections.indexid",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIntegrityError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [17], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m transform \u001b[38;5;241m=\u001b[39m Transform(embeddings, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tempfile\u001b[38;5;241m.\u001b[39mNamedTemporaryFile(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m, suffix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m buffer:\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;66;03m# Load documents into database and transform to vectors\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m         ids, dimensions, vecs \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m ids:\n\u001b[1;32m     13\u001b[0m             \u001b[38;5;66;03m# Normalize embeddings\u001b[39;00m\n\u001b[1;32m     14\u001b[0m             embeddings\u001b[38;5;241m.\u001b[39mnormalize(vecs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/txtai/embeddings/transform.py:58\u001b[0m, in \u001b[0;36mTransform.__call__\u001b[0;34m(self, documents, buffer)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03mProcesses an iterable collection of documents, handles any iterable including generators.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03m    (document ids, dimensions, embeddings)\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Transform documents to vectors and load into database\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m ids, dimensions, batches, stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Check that embeddings are available and load as a memmap\u001b[39;00m\n\u001b[1;32m     61\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/txtai/vectors/base.py:79\u001b[0m, in \u001b[0;36mVectors.index\u001b[0;34m(self, documents, batchsize)\u001b[0m\n\u001b[1;32m     77\u001b[0m stream \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m     78\u001b[0m batch \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m document \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[1;32m     80\u001b[0m     batch\u001b[38;5;241m.\u001b[39mappend(document)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch) \u001b[38;5;241m==\u001b[39m batchsize:\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;66;03m# Convert batch to embeddings\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/txtai/embeddings/transform.py:117\u001b[0m, in \u001b[0;36mTransform.stream\u001b[0;34m(self, documents)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Final batch\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/txtai/embeddings/transform.py:142\u001b[0m, in \u001b[0;36mTransform.load\u001b[0;34m(self, batch, offset)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# Load batch into database except if this is a reindex\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatabase \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction \u001b[38;5;241m!=\u001b[39m Action\u001b[38;5;241m.\u001b[39mREINDEX:\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# Load batch into graph\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/txtai/database/sqlite.py:151\u001b[0m, in \u001b[0;36mSQLite.insert\u001b[0;34m(self, documents, index)\u001b[0m\n\u001b[1;32m    148\u001b[0m     document \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# Save text section\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsertsection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/txtai/database/sqlite.py:433\u001b[0m, in \u001b[0;36mSQLite.insertsection\u001b[0;34m(self, index, uid, text, tags, entry)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;124;03mInserts a section.\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;124;03m    entry: generated entry date\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;66;03m# Save text section\u001b[39;00m\n\u001b[0;32m--> 433\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSQLite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mINSERT_SECTION\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIntegrityError\u001b[0m: UNIQUE constraint failed: sections.indexid"
     ]
    }
   ],
   "source": [
    "from txtai.embeddings.transform import Transform\n",
    "import tempfile\n",
    "from uuid import uuid4\n",
    "documents = [\n",
    "    (str(uuid4()), doc['text'], None) \n",
    "    for uid, doc in data.items()\n",
    "]\n",
    "transform = Transform(embeddings, 3)\n",
    "with tempfile.NamedTemporaryFile(mode=\"wb\", suffix=\".npy\") as buffer:\n",
    "        # Load documents into database and transform to vectors\n",
    "        ids, dimensions, vecs = transform(documents, buffer)\n",
    "        if ids:\n",
    "            # Normalize embeddings\n",
    "            embeddings.normalize(vecs)\n",
    "\n",
    "            # Save index dimensions\n",
    "            embeddings.config[\"dimensions\"] = dimensions\n",
    "vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b12f4be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.search(\"select id, text, obj_id, score from txtai where similar('ice shelf')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1c925c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.save('./tmp_stuff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7a4ced8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Embeddings' object has no attribute 'embeddings'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Embeddings' object has no attribute 'embeddings'"
     ]
    }
   ],
   "source": [
    "embeddings.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d23a5a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
